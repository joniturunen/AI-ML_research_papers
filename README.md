# AI-ML_research_papers

Looking for the collection? It is [right here on this page](#collection)

Repository is a a collection of AI/ML-related research papers publicly available via [arXiv.org](https://arxiv.org). *Rationale* for the repository is to share (relevant in the mind of the author) resources easily with other people.

> To maintain a single source of truth, this repository links to the material instead of redistributing it.

The collection follows loosely Harvard style citation with [GitHub markdown language](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) using [LaTeX](https://www.latex-project.org) style and [Markdown Badges](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/creating-a-custom-badge-for-your-github-app) to display [Zenodo DOI badge](https://zenodo.org).

---

## Collection

<span id="ref-1"></span>[**[1]**](#ref-1) *Shapiro, D. et al.* (2023) **Conceptual Framework for Autonomous Cognitive Entities**. arXiv.org.  
[![DOI:10.1007/978-3-031-21438-7_60](https://zenodo.org/badge/DOI/10.13140/RG.2.2.14161.30569.svg)](https://doi.org/10.13140/RG.2.2.14161.30569)  
[![arXiv](https://img.shields.io/badge/arXiv-2310.06775-<COLOR>.svg)](https://arxiv.org/abs/2310.06775)

<span id="ref-2"></span>[**[2]**](#ref-2) *Wu, Q. et al.* (2023) **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation**. arXiv.org.  
[![DOI:10.48550/arxiv.2308.08155](https://zenodo.org/badge/DOI/10.48550/arxiv.2308.08155.svg)](https://doi.org/10.48550/arxiv.2308.08155)  
[![arXiv](https://img.shields.io/badge/arXiv-2308.08155-<COLOR>.svg)](https://arxiv.org/abs/2308.08155)

<span id="ref-3"></span>[**[3]**](#ref-3) *Dell'Acqua F. et al.* (2023) **Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality**. Harvard Business School Technology & Operations Mgt. Unit Working Paper No. 24-013.  
[![DOI:10.2139/ssrn.4573321](https://zenodo.org/badge/DOI/10.2139/ssrn.4573321.svg)](http://dx.doi.org/10.2139/ssrn.4573321)üî•

<span id="ref-4"></span>[**[4]**](#ref-4) *Packer C et al.* (2023) **MemGPT: Towards LLMs as Operating Systems**. arXiv.  
[![DOI:10.48550/arxiv.2310.08560](https://zenodo.org/badge/DOI/10.48550/arxiv.2310.08560.svg)](https://doi.org/10.48550/arxiv.2310.08560)üî•üß†

<span id="ref-5"></span>[**[5]**](#ref-5) *Ebert, Christof, et al.* (2023) **Generative AI for Software Practitioners**. IEEE Software.  
[![DOI:10.1109/ms.2023.3265877](https://zenodo.org/badge/DOI/10.1109/ms.2023.3265877.svg)](http://dx.doi.org/10.1109/ms.2023.3265877)

<span id="ref-6"></span>[**[6]**](#ref-6) *Peng S et al.* (2023) **The Impact of AI on Developer Productivity: Evidence from GitHub Copilot**. arXiv.  
[![DOI:10.48550/arxiv.2302.06590](https://zenodo.org/badge/DOI/10.48550/arxiv.2302.06590.svg)](https://doi.org/10.48550/arxiv.2302.06590)

<span id="ref-7"></span>[**[7]**](#ref-7) *Wang R et al.* (2023) **Investigating and Designing for Trust in AI-powered Code Generation Tools**. arXiv.  
[![DOI:10.48550/arxiv.2305.11248](https://zenodo.org/badge/DOI/10.48550/arxiv.2305.11248.svg)](https://doi.org/10.48550/arxiv.2305.11248)

<span id="ref-8"></span>[**[8]**](#ref-8) *Dettmers T et al.* (2023) **QLoRA: Efficient Finetuning of Quantized LLMs**. arXiv.  
[![DOI:10.48550/arxiv.2305.14314](https://zenodo.org/badge/DOI/10.48550/arxiv.2305.14314.svg)](https://doi.org/10.48550/arxiv.2305.14314)üî•üéõÔ∏è

<span id="ref-9"></span>[**[9]**](#ref-9) *Ding J et al.* (2023) **LongNet: Scaling Transformers to 1,000,000,000 Tokens**. arXiv.  
[![DOI:10.48550/arxiv.2307.02486](https://zenodo.org/badge/DOI/10.48550/arxiv.2307.02486.svg)](https://doi.org/10.48550/arxiv.2307.02486)

<span id="ref-10"></span>[**[10]**](#ref-10) *Feuerriegel S et al.* (2023) **Generative AI**. Business &amp; Information Systems Engineering.  
[![DOI:10.1007/s12599-023-00834-7](https://zenodo.org/badge/DOI/10.1007/s12599-023-00834-7.svg)](http://dx.doi.org/10.1007/s12599-023-00834-7)

<span id="ref-11"></span>[**[11]**](#ref-11) *Touvron H et al.* (2023) **Llama 2: Open Foundation and Fine-Tuned Chat Models**. arXiv.  
[![DOI:10.48550/arxiv.2307.09288](https://zenodo.org/badge/DOI/10.48550/arxiv.2307.09288.svg)](https://doi.org/10.48550/arxiv.2307.09288)üî•ü¶ô

